# ğŸ§  í•™ìŠµ ì‹œìŠ¤í…œ ì„¤ê³„ì„œ

> **í•µì‹¬ ì² í•™**: "ì‹œì¥ì€ ë³€í•˜ê³ , ì „ëµë„ ì§„í™”í•´ì•¼ í•œë‹¤"
> **ëª©í‘œ**: ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì„±ê³¼ê°€ ê°œì„ ë˜ëŠ” ìê¸° í•™ìŠµ ì‹œìŠ¤í…œ

---

## 1. í•™ìŠµ ì‹œìŠ¤í…œ ê°œìš”

### 1.1 í•™ìŠµì´ í•„ìš”í•œ ì´ìœ 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì™œ í•™ìŠµì´ í•„ìš”í•œê°€?                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ì •ì  ì „ëµì˜ í•œê³„:                                               â”‚
â”‚  â”œâ”€ ê³¼ê±° ë°ì´í„°ì— ê³¼ìµœì í™”                                       â”‚
â”‚  â”œâ”€ ì‹œì¥ êµ¬ì¡° ë³€í™”ì— ì ì‘ ë¶ˆê°€                                   â”‚
â”‚  â”œâ”€ ìƒˆë¡œìš´ íŒ¨í„´ í•™ìŠµ ë¶ˆê°€                                        â”‚
â”‚  â””â”€ ì„±ê³¼ ì €í•˜ ì‹œ ì›ì¸ íŒŒì•… ì–´ë ¤ì›€                                â”‚
â”‚                                                                 â”‚
â”‚  í•™ìŠµ ì‹œìŠ¤í…œì˜ ì¥ì :                                             â”‚
â”‚  â”œâ”€ ì§€ì†ì ì¸ ì„±ê³¼ ê°œì„                                           â”‚
â”‚  â”œâ”€ ì‹œì¥ ë³€í™” ìë™ ì ì‘                                          â”‚
â”‚  â”œâ”€ ì‹¤ìˆ˜ì—ì„œ í•™ìŠµ                                                â”‚
â”‚  â””â”€ ì „ëµ ìë™ ìµœì í™”                                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 í•™ìŠµ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         í•™ìŠµ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        ë°ì´í„° ìˆ˜ì§‘ ê³„ì¸µ                           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚  â”‚ ì‹œì¥    â”‚  â”‚ ê±°ë˜    â”‚  â”‚ ì‹ í˜¸    â”‚  â”‚ ê²°ê³¼    â”‚             â”‚  â”‚
â”‚  â”‚  â”‚ ë°ì´í„°  â”‚  â”‚ ë¡œê·¸    â”‚  â”‚ ê¸°ë¡    â”‚  â”‚ í”¼ë“œë°±  â”‚             â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚            â”‚            â”‚            â”‚                      â”‚
â”‚          â–¼            â–¼            â–¼            â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                       ë¶„ì„ ë° í•™ìŠµ ê³„ì¸µ                           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚  â”‚
â”‚  â”‚  â”‚   íŒ¨í„´      â”‚  â”‚   ì„±ê³¼      â”‚  â”‚   ì‹¤íŒ¨      â”‚               â”‚  â”‚
â”‚  â”‚  â”‚   í•™ìŠµê¸°    â”‚  â”‚   ë¶„ì„ê¸°    â”‚  â”‚   ë¶„ì„ê¸°    â”‚               â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                â”‚                â”‚                          â”‚
â”‚            â–¼                â–¼                â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        ì ì‘ ë° ìµœì í™” ê³„ì¸µ                        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚  â”‚
â”‚  â”‚  â”‚   íŒŒë¼ë¯¸í„°  â”‚  â”‚   ê°€ì¤‘ì¹˜    â”‚  â”‚   ì „ëµ      â”‚               â”‚  â”‚
â”‚  â”‚  â”‚   ì¡°ì •ê¸°    â”‚  â”‚   ì¡°ì •ê¸°    â”‚  â”‚   ì„ íƒê¸°    â”‚               â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                â”‚                â”‚                          â”‚
â”‚            â–¼                â–¼                â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        ì‹¤í–‰ ê³„ì¸µ                                  â”‚  â”‚
â”‚  â”‚           ì—…ë°ì´íŠ¸ëœ ì „ëµ â†’ ì‹¤ì‹œê°„ ì ìš© â†’ ê²°ê³¼ í”¼ë“œë°±             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ê±°ë˜ ê²°ê³¼ í•™ìŠµ

### 2.1 ê±°ë˜ ë¡œê·¸ ìˆ˜ì§‘

```python
class TradeLogger:
    """
    ëª¨ë“  ê±°ë˜ì˜ ìƒì„¸ ì •ë³´ ê¸°ë¡

    ê¸°ë¡ í•­ëª©:
    - ì§„ì…/ì²­ì‚° ì‹œì ì˜ ëª¨ë“  ì§€í‘œ
    - ì‹ í˜¸ ë°œìƒ ê·¼ê±°
    - ì‹œì¥ ìƒí™©
    - ê²°ê³¼ ë° ì›ì¸ ë¶„ì„
    """

    def log_trade(self, trade: Trade, context: TradeContext) -> TradeLog:
        """
        ê±°ë˜ ìƒì„¸ ë¡œê·¸ ìƒì„±

        ê¸°ë¡ í•­ëª© (í•™ìŠµì— í™œìš©):
        1. ì§„ì… ì‹œì  ìƒíƒœ
        2. ì²­ì‚° ì‹œì  ìƒíƒœ
        3. ë³´ìœ  ê¸°ê°„ ì¤‘ ë³€í™”
        4. ê²°ê³¼ ë° ì›ì¸
        """
        log = TradeLog(
            trade_id=trade.id,
            timestamp=datetime.now(),

            # ê¸°ë³¸ ì •ë³´
            stock_code=trade.stock_code,
            direction=trade.direction,
            entry_price=trade.entry_price,
            exit_price=trade.exit_price,
            quantity=trade.quantity,
            pnl=trade.pnl,
            pnl_pct=trade.pnl_pct,
            holding_days=trade.holding_days,

            # ì§„ì… ì‹œì  ìƒíƒœ
            entry_context=EntryContext(
                # ê¸°ìˆ ì  ì§€í‘œ
                rsi=context.entry_indicators['rsi'],
                macd=context.entry_indicators['macd'],
                macd_signal=context.entry_indicators['macd_signal'],
                bb_position=context.entry_indicators['bb_position'],
                ma_trend=context.entry_indicators['ma_trend'],
                volume_ratio=context.entry_indicators['volume_ratio'],

                # ì‹ í˜¸ ì •ë³´
                signal_source=context.signal_source,  # ['LSTM', 'TA', 'SD']
                signal_strength=context.signal_strength,
                signal_confidence=context.signal_confidence,
                agreement_count=context.agreement_count,

                # ì‹œì¥ ìƒí™©
                market_regime=context.market_regime,
                sector_rank=context.sector_rank,
                vix_level=context.vix_level,

                # MTF ìƒíƒœ
                daily_trend=context.daily_trend,
                weekly_trend=context.weekly_trend,
                monthly_trend=context.monthly_trend,
            ),

            # ì²­ì‚° ì‹œì  ìƒíƒœ
            exit_context=ExitContext(
                exit_reason=trade.exit_reason,  # signal, stop_loss, take_profit, trailing, timeout
                indicators_at_exit=context.exit_indicators,
                market_regime_at_exit=context.exit_market_regime,
                max_profit_during=context.max_profit_during,
                max_loss_during=context.max_loss_during,
            ),

            # ë¶„ë¥˜ ë ˆì´ë¸” (í•™ìŠµìš©)
            labels=TradeLabels(
                is_winner=trade.pnl > 0,
                is_big_winner=trade.pnl_pct > 5,
                is_big_loser=trade.pnl_pct < -3,
                exit_optimal=self._evaluate_exit_timing(trade, context),
                entry_optimal=self._evaluate_entry_timing(trade, context),
            )
        )

        # DBì— ì €ì¥
        self.db.save_trade_log(log)

        return log

    def _evaluate_exit_timing(self, trade: Trade, context: TradeContext) -> str:
        """
        ì²­ì‚° íƒ€ì´ë° í‰ê°€

        ë¶„ë¥˜:
        - 'optimal': ìµœì  íƒ€ì´ë° (ê³ ì /ì €ì  ê·¼ì²˜)
        - 'early': ë„ˆë¬´ ì¼ì° (ë” ê°ˆ ìˆ˜ ìˆì—ˆìŒ)
        - 'late': ë„ˆë¬´ ëŠ¦ìŒ (ìˆ˜ìµ ë°˜ë‚©)
        - 'neutral': ì ì ˆí•¨
        """
        if trade.pnl > 0:  # ìˆ˜ìµ ê±°ë˜
            # ë³´ìœ  ì¤‘ ìµœê³ ì  ëŒ€ë¹„ ì²­ì‚°ê°€
            peak_to_exit = (context.max_profit_during - trade.pnl_pct) / context.max_profit_during

            if peak_to_exit < 0.1:  # ìµœê³ ì  ëŒ€ë¹„ 10% ì´ë‚´ ì²­ì‚°
                return 'optimal'
            elif peak_to_exit > 0.5:  # ìµœê³ ì  ëŒ€ë¹„ 50% ì´ìƒ ë°˜ë‚©
                return 'late'
            else:
                return 'neutral'
        else:  # ì†ì‹¤ ê±°ë˜
            # ì†ì ˆ ì ì‹œì„±
            if trade.exit_reason == 'stop_loss':
                return 'neutral'  # ì†ì ˆ ê·œì¹™ ì¤€ìˆ˜
            elif context.max_loss_during < trade.pnl_pct:  # ë” í° ì†ì‹¤ ë°©ì§€
                return 'optimal'
            else:
                return 'late'
```

### 2.2 ì„±ê³¼ íŒ¨í„´ ë¶„ì„

```python
class PerformancePatternAnalyzer:
    """
    ê±°ë˜ ì„±ê³¼ íŒ¨í„´ ë¶„ì„

    ë¶„ì„ ê´€ì :
    1. ì–´ë–¤ ì¡°ê±´ì—ì„œ ìŠ¹ë¦¬í•˜ëŠ”ê°€?
    2. ì–´ë–¤ ì¡°ê±´ì—ì„œ íŒ¨ë°°í•˜ëŠ”ê°€?
    3. ìŠ¹ë¦¬/íŒ¨ë°° íŒ¨í„´ì˜ ê³µí†µì ì€?
    """

    def analyze_patterns(self, trade_logs: List[TradeLog]) -> PatternAnalysis:
        """
        ê±°ë˜ íŒ¨í„´ ì¢…í•© ë¶„ì„
        """
        winners = [t for t in trade_logs if t.labels.is_winner]
        losers = [t for t in trade_logs if not t.labels.is_winner]

        return PatternAnalysis(
            # ìŠ¹ë¦¬ ì¡°ê±´ ë¶„ì„
            winning_conditions=self._analyze_winning_conditions(winners),

            # íŒ¨ë°° ì¡°ê±´ ë¶„ì„
            losing_conditions=self._analyze_losing_conditions(losers),

            # ì§€í‘œë³„ ìµœì  ë²”ìœ„
            optimal_indicator_ranges=self._find_optimal_ranges(trade_logs),

            # ì‹œì¥ ìƒí™©ë³„ ì„±ê³¼
            performance_by_regime=self._analyze_by_regime(trade_logs),

            # ì‹ í˜¸ ì†ŒìŠ¤ë³„ ì„±ê³¼
            performance_by_source=self._analyze_by_source(trade_logs),

            # ìš”ì¼/ì‹œê°„ë³„ ì„±ê³¼
            performance_by_time=self._analyze_by_time(trade_logs),

            # ë³´ìœ  ê¸°ê°„ë³„ ì„±ê³¼
            performance_by_holding=self._analyze_by_holding_period(trade_logs),
        )

    def _analyze_winning_conditions(self, winners: List[TradeLog]) -> WinningConditions:
        """
        ìŠ¹ë¦¬ ê±°ë˜ì˜ ê³µí†µ ì¡°ê±´ ë¶„ì„

        í•™ìŠµ ëª©í‘œ:
        - ë†’ì€ ìŠ¹ë¥ ì˜ ì§„ì… ì¡°ê±´ ì‹ë³„
        - ì„±ê³µì ì¸ ì²­ì‚° íŒ¨í„´ íŒŒì•…
        """
        if not winners:
            return WinningConditions()

        # RSI ë¶„í¬
        rsi_at_entry = [t.entry_context.rsi for t in winners]
        rsi_mean = np.mean(rsi_at_entry)
        rsi_std = np.std(rsi_at_entry)

        # ì‹ í˜¸ ì¼ì¹˜ ìˆ˜
        agreement_counts = [t.entry_context.agreement_count for t in winners]
        avg_agreement = np.mean(agreement_counts)

        # ì‹œì¥ ë ˆì§ ë¶„í¬
        regime_dist = Counter([t.entry_context.market_regime for t in winners])
        best_regime = regime_dist.most_common(1)[0][0]

        # MTF ì •ë ¬
        aligned_count = sum(1 for t in winners
                          if t.entry_context.daily_trend == t.entry_context.weekly_trend)
        alignment_rate = aligned_count / len(winners)

        # ì‹ í˜¸ ì†ŒìŠ¤
        source_dist = Counter([
            tuple(sorted(t.entry_context.signal_source)) for t in winners
        ])
        best_source_combo = source_dist.most_common(1)[0][0]

        return WinningConditions(
            rsi_range=(rsi_mean - rsi_std, rsi_mean + rsi_std),
            min_agreement=int(avg_agreement),
            best_regime=best_regime,
            mtf_alignment_rate=alignment_rate,
            best_source_combo=list(best_source_combo),

            # í†µê³„
            total_winners=len(winners),
            avg_profit=np.mean([t.pnl_pct for t in winners]),
            avg_holding_days=np.mean([t.holding_days for t in winners]),
        )

    def _find_optimal_ranges(self, trade_logs: List[TradeLog]) -> dict:
        """
        ì§€í‘œë³„ ìµœì  ë²”ìœ„ íƒìƒ‰

        ë°©ë²•: ìŠ¹ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ì§€í‘œ ë²”ìœ„ íƒìƒ‰
        """
        indicators = ['rsi', 'macd', 'bb_position', 'volume_ratio']
        optimal_ranges = {}

        for indicator in indicators:
            values = [(getattr(t.entry_context, indicator), t.labels.is_winner)
                     for t in trade_logs
                     if hasattr(t.entry_context, indicator)]

            if not values:
                continue

            # êµ¬ê°„ë³„ ìŠ¹ë¥  ê³„ì‚°
            df = pd.DataFrame(values, columns=['value', 'is_winner'])
            df['bin'] = pd.qcut(df['value'], q=10, duplicates='drop')

            bin_stats = df.groupby('bin').agg({
                'is_winner': ['mean', 'count']
            })

            # ìŠ¹ë¥  60% ì´ìƒì¸ êµ¬ê°„
            good_bins = bin_stats[bin_stats[('is_winner', 'mean')] >= 0.6]

            if not good_bins.empty:
                optimal_ranges[indicator] = {
                    'ranges': [(b.left, b.right) for b in good_bins.index],
                    'win_rate': good_bins[('is_winner', 'mean')].mean(),
                    'sample_size': good_bins[('is_winner', 'count')].sum()
                }

        return optimal_ranges
```

### 2.3 ì‹¤íŒ¨ ë¶„ì„ ë° í•™ìŠµ

```python
class FailureAnalyzer:
    """
    ì‹¤íŒ¨ ê±°ë˜ ì‹¬ì¸µ ë¶„ì„

    ëª©í‘œ:
    - ê°™ì€ ì‹¤ìˆ˜ ë°˜ë³µ ë°©ì§€
    - ì‹¤íŒ¨ íŒ¨í„´ ì‹ë³„ ë° í•„í„°ë§
    - ì†ì ˆ ê°œì„ ì  ë„ì¶œ
    """

    def analyze_failures(self, losers: List[TradeLog]) -> FailureAnalysis:
        """
        ì‹¤íŒ¨ ê±°ë˜ ë¶„ì„
        """
        # ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜
        failure_types = self._classify_failures(losers)

        # ê° ìœ í˜•ë³„ ë¶„ì„
        analyses = {}
        for failure_type, trades in failure_types.items():
            analyses[failure_type] = self._analyze_failure_type(failure_type, trades)

        return FailureAnalysis(
            failure_distribution=failure_types,
            type_analyses=analyses,
            common_mistakes=self._find_common_mistakes(losers),
            improvement_suggestions=self._generate_improvements(analyses)
        )

    def _classify_failures(self, losers: List[TradeLog]) -> dict:
        """
        ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜

        ìœ í˜•:
        1. TREND_AGAINST: ì¶”ì„¸ ì—­í–‰ ì§„ì…
        2. EARLY_ENTRY: ë„ˆë¬´ ì´ë¥¸ ì§„ì…
        3. LATE_ENTRY: ë„ˆë¬´ ëŠ¦ì€ ì§„ì…
        4. BAD_TIMING: ì‹œì¥ íƒ€ì´ë° ì‹¤íŒ¨
        5. STOP_TOO_TIGHT: ì†ì ˆ ë„ˆë¬´ íƒ€ì´íŠ¸
        6. STOP_TOO_WIDE: ì†ì ˆ ë„ˆë¬´ ë„“ìŒ
        7. WEAK_SIGNAL: ì•½í•œ ì‹ í˜¸ì— ì§„ì…
        """
        classified = defaultdict(list)

        for trade in losers:
            # ì¶”ì„¸ ì—­í–‰ ì²´í¬
            if (trade.entry_context.daily_trend != trade.entry_context.weekly_trend):
                classified['TREND_AGAINST'].append(trade)

            # ì‹ í˜¸ ê°•ë„ ì²´í¬
            elif trade.entry_context.signal_confidence < 0.6:
                classified['WEAK_SIGNAL'].append(trade)

            # ì‹ í˜¸ ì¼ì¹˜ ë¶€ì¡±
            elif trade.entry_context.agreement_count < 2:
                classified['LOW_AGREEMENT'].append(trade)

            # ì†ì ˆ íƒ€ì´ë° ë¶„ì„
            elif trade.exit_reason == 'stop_loss':
                if trade.entry_context.max_loss_during > trade.pnl_pct * 1.5:
                    classified['STOP_TOO_TIGHT'].append(trade)
                else:
                    classified['MARKET_MOVED'].append(trade)

            else:
                classified['OTHER'].append(trade)

        return dict(classified)

    def _generate_improvements(self, analyses: dict) -> List[Improvement]:
        """
        ê°œì„ ì  ë„ì¶œ

        ê° ì‹¤íŒ¨ ìœ í˜•ì— ëŒ€í•œ êµ¬ì²´ì  ê°œì„  ë°©ì•ˆ
        """
        improvements = []

        if 'TREND_AGAINST' in analyses:
            analysis = analyses['TREND_AGAINST']
            if analysis['count'] > 5:
                improvements.append(Improvement(
                    priority='high',
                    category='entry_filter',
                    action='ADD_MTF_ALIGNMENT_CHECK',
                    description="MTF ì •ë ¬ í•„í„° ì¶”ê°€: ì¼/ì£¼ë´‰ ì¶”ì„¸ ì¼ì¹˜ ì‹œì—ë§Œ ì§„ì…",
                    expected_impact=f"ì˜ˆìƒ ìŠ¹ë¥  ê°œì„ : +{analysis['potential_win_rate_gain']:.1%}",
                    implementation="""
                    if daily_trend != weekly_trend:
                        return Signal(type=HOLD, reason='MTF_NOT_ALIGNED')
                    """
                ))

        if 'WEAK_SIGNAL' in analyses:
            analysis = analyses['WEAK_SIGNAL']
            improvements.append(Improvement(
                priority='high',
                category='signal_filter',
                action='INCREASE_CONFIDENCE_THRESHOLD',
                description="ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’ ìƒí–¥: 60% â†’ 70%",
                expected_impact=f"ì•½ {analysis['count']}ê±´ ì†ì‹¤ íšŒí”¼ ê°€ëŠ¥",
                implementation="""
                self.min_confidence = 0.70  # ê¸°ì¡´ 0.60
                """
            ))

        if 'LOW_AGREEMENT' in analyses:
            analysis = analyses['LOW_AGREEMENT']
            improvements.append(Improvement(
                priority='medium',
                category='signal_filter',
                action='REQUIRE_MORE_AGREEMENT',
                description="ìµœì†Œ ì‹ í˜¸ ì¼ì¹˜ ìˆ˜ ìƒí–¥: 2ê°œ â†’ 3ê°œ",
                expected_impact=f"ê±°ë˜ ìˆ˜ ê°ì†Œí•˜ë‚˜ ìŠ¹ë¥  ê°œì„ ",
                implementation="""
                self.min_agreement = 3  # ê¸°ì¡´ 2
                """
            ))

        return sorted(improvements, key=lambda x: x.priority)
```

---

## 3. ëª¨ë¸ ì§€ì† í•™ìŠµ

### 3.1 LSTM ëª¨ë¸ ì¬í•™ìŠµ

```python
class LSTMContinuousLearner:
    """
    LSTM ëª¨ë¸ ì§€ì† í•™ìŠµ ì‹œìŠ¤í…œ

    í•™ìŠµ ì „ëµ:
    1. ì •ê¸° ì¬í•™ìŠµ (ì£¼ê°„/ì›”ê°„)
    2. ì„±ê³¼ ì €í•˜ ì‹œ ì¦‰ì‹œ ì¬í•™ìŠµ
    3. ì‹œì¥ ë ˆì§ ë³€í™” ì‹œ ì¬í•™ìŠµ
    """

    def __init__(self):
        self.retrain_period = 'weekly'     # ì •ê¸° ì¬í•™ìŠµ ì£¼ê¸°
        self.min_new_samples = 100         # ìµœì†Œ ì‹ ê·œ ìƒ˜í”Œ
        self.performance_threshold = 0.55  # ì„±ê³¼ ì„ê³„ê°’ (55% ë¯¸ë§Œì‹œ ì¬í•™ìŠµ)
        self.lookback_days = 252           # í•™ìŠµ ë°ì´í„° ê¸°ê°„ (1ë…„)

    def should_retrain(self, current_performance: dict) -> RetrainDecision:
        """
        ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ íŒë‹¨
        """
        reasons = []

        # 1. ì •ê¸° ì¬í•™ìŠµ ì²´í¬
        if self._is_scheduled_retrain_time():
            reasons.append('scheduled')

        # 2. ì„±ê³¼ ì €í•˜ ì²´í¬
        recent_accuracy = current_performance.get('recent_accuracy', 0)
        if recent_accuracy < self.performance_threshold:
            reasons.append(f'low_performance ({recent_accuracy:.1%})')

        # 3. ì‹œì¥ ë ˆì§ ë³€í™” ì²´í¬
        if self._detect_regime_change():
            reasons.append('regime_change')

        # 4. ì‹ ê·œ ë°ì´í„° ì¶©ë¶„ ì²´í¬
        new_samples = self._count_new_samples()
        if new_samples >= self.min_new_samples:
            reasons.append(f'sufficient_new_data ({new_samples})')

        should_retrain = len(reasons) > 0

        return RetrainDecision(
            should_retrain=should_retrain,
            reasons=reasons,
            urgency='high' if 'low_performance' in str(reasons) else 'normal'
        )

    def retrain(self, force: bool = False) -> RetrainResult:
        """
        ëª¨ë¸ ì¬í•™ìŠµ ì‹¤í–‰

        ì ˆì°¨:
        1. ë°ì´í„° ì¤€ë¹„ (ìµœê·¼ 1ë…„)
        2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        3. í•™ìŠµ/ê²€ì¦ ë¶„í• 
        4. ëª¨ë¸ í•™ìŠµ
        5. ì„±ëŠ¥ ê²€ì¦
        6. ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ
        7. ì¡°ê±´ ì¶©ì¡± ì‹œ êµì²´
        """
        # 1. ë°ì´í„° ì¤€ë¹„
        training_data = self._prepare_training_data()

        # 2. í”¼ì²˜ ìƒì„±
        X, y = self._create_features(training_data)

        # 3. í•™ìŠµ/ê²€ì¦ ë¶„í•  (ì‹œê³„ì—´ ë¶„í• )
        X_train, X_val, y_train, y_val = self._time_series_split(X, y)

        # 4. ëª¨ë¸ í•™ìŠµ
        new_model = self._train_model(X_train, y_train)

        # 5. ê²€ì¦
        val_accuracy = self._evaluate(new_model, X_val, y_val)
        old_accuracy = self._evaluate(self.current_model, X_val, y_val)

        # 6. ë¹„êµ ë° êµì²´ ê²°ì •
        improvement = val_accuracy - old_accuracy

        result = RetrainResult(
            new_accuracy=val_accuracy,
            old_accuracy=old_accuracy,
            improvement=improvement,
            training_samples=len(X_train),
            validation_samples=len(X_val),
        )

        # 7. êµì²´ ì¡°ê±´: ê°œì„  ë˜ëŠ” ê¸°ì¡´ ëŒ€ë¹„ 95% ì´ìƒ ì„±ëŠ¥
        if improvement > 0 or val_accuracy >= old_accuracy * 0.95:
            self._replace_model(new_model)
            result.model_replaced = True
            result.replacement_reason = 'improved' if improvement > 0 else 'acceptable'
        else:
            result.model_replaced = False
            result.replacement_reason = 'not_improved'

        return result

    def _create_features(self, data: pd.DataFrame) -> Tuple[np.array, np.array]:
        """
        í•™ìŠµìš© í”¼ì²˜ ìƒì„±

        í”¼ì²˜ ì¹´í…Œê³ ë¦¬:
        1. ê°€ê²© ê¸°ë°˜ (ìˆ˜ìµë¥ , ë³€ë™ì„±)
        2. ê¸°ìˆ ì  ì§€í‘œ (RSI, MACD, BB ë“±)
        3. ê±°ë˜ëŸ‰ ê¸°ë°˜ (ê±°ë˜ëŸ‰ ë¹„ìœ¨, ì¶”ì„¸)
        4. ì‹œì¥ ìƒí™© (ë ˆì§, VIX)
        5. ì‹œê°„ ê¸°ë°˜ (ìš”ì¼, ì›”)
        """
        features = []

        for i in range(self.sequence_length, len(data)):
            window = data.iloc[i-self.sequence_length:i]

            feature_vector = np.concatenate([
                # ê°€ê²© í”¼ì²˜
                self._price_features(window),

                # ê¸°ìˆ ì  ì§€í‘œ
                self._technical_features(window),

                # ê±°ë˜ëŸ‰ í”¼ì²˜
                self._volume_features(window),

                # ì‹œì¥ í”¼ì²˜
                self._market_features(window),
            ])

            features.append(feature_vector)

        # ë ˆì´ë¸”: ë‹¤ìŒ Nì¼ ìˆ˜ìµë¥  (ì–‘ìˆ˜/ìŒìˆ˜)
        labels = (data['close'].pct_change(self.prediction_horizon)
                  .shift(-self.prediction_horizon)
                  .iloc[self.sequence_length:-self.prediction_horizon] > 0).astype(int)

        return np.array(features), np.array(labels)
```

### 3.2 ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ

```python
class EnsembleWeightLearner:
    """
    ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìë™ í•™ìŠµ

    í•™ìŠµ ë°©ë²•:
    1. ìµœê·¼ ì„±ê³¼ ê¸°ë°˜ ì¡°ì •
    2. ì‹œì¥ ìƒí™©ë³„ ìµœì  ê°€ì¤‘ì¹˜
    3. ë² ì´ì§€ì•ˆ ìµœì í™”
    """

    def __init__(self):
        self.evaluation_window = 20  # í‰ê°€ ê¸°ê°„ (20 ê±°ë˜ì¼)
        self.min_weight = 0.15       # ìµœì†Œ ê°€ì¤‘ì¹˜
        self.max_weight = 0.50       # ìµœëŒ€ ê°€ì¤‘ì¹˜
        self.learning_rate = 0.1     # í•™ìŠµë¥ 

    def learn_weights(self,
                     strategy_signals: Dict[str, List[Signal]],
                     actual_results: List[TradeResult]) -> Dict[str, float]:
        """
        ì „ëµë³„ ìµœì  ê°€ì¤‘ì¹˜ í•™ìŠµ

        ë°©ë²•: ê° ì „ëµì˜ ì‹ í˜¸ ì •í™•ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
        """
        # ê° ì „ëµ ì„±ê³¼ í‰ê°€
        strategy_performance = {}

        for strategy_name, signals in strategy_signals.items():
            # ì‹ í˜¸ì™€ ì‹¤ì œ ê²°ê³¼ ë§¤ì¹­
            matches = self._match_signals_to_results(signals, actual_results)

            # ì •í™•ë„ ê³„ì‚°
            accuracy = self._calculate_accuracy(matches)
            profit_factor = self._calculate_profit_factor(matches)
            sharpe = self._calculate_sharpe(matches)

            # ì¢…í•© ì ìˆ˜
            score = (accuracy * 0.4 + profit_factor * 0.3 + sharpe * 0.3)
            strategy_performance[strategy_name] = score

        # ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        total_score = sum(strategy_performance.values())
        raw_weights = {k: v / total_score for k, v in strategy_performance.items()}

        # ì œí•œ ë²”ìœ„ ì ìš©
        adjusted_weights = {}
        for strategy, weight in raw_weights.items():
            adjusted = max(self.min_weight, min(self.max_weight, weight))
            adjusted_weights[strategy] = adjusted

        # ì •ê·œí™”
        total = sum(adjusted_weights.values())
        final_weights = {k: v / total for k, v in adjusted_weights.items()}

        return final_weights

    def learn_regime_specific_weights(self,
                                     historical_data: Dict,
                                     regimes: List[MarketRegime]) -> Dict:
        """
        ì‹œì¥ ë ˆì§ë³„ ìµœì  ê°€ì¤‘ì¹˜ í•™ìŠµ

        ê° ë ˆì§ì—ì„œ ì–´ë–¤ ì „ëµì´ ì˜ ì‘ë™í•˜ëŠ”ì§€ í•™ìŠµ
        """
        regime_weights = {}

        for regime in set(regimes):
            # í•´ë‹¹ ë ˆì§ ê¸°ê°„ ë°ì´í„° í•„í„°ë§
            regime_data = self._filter_by_regime(historical_data, regime)

            if len(regime_data) < 50:  # ìµœì†Œ ìƒ˜í”Œ
                continue

            # í•´ë‹¹ ë ˆì§ì—ì„œ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰
            optimal_weights = self._optimize_weights_for_regime(regime_data)
            regime_weights[regime] = optimal_weights

        return regime_weights

    def _optimize_weights_for_regime(self, regime_data: Dict) -> Dict[str, float]:
        """
        ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ íŠ¹ì • ë ˆì§ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰
        """
        from scipy.optimize import minimize

        strategies = list(regime_data['signals'].keys())
        n = len(strategies)

        def objective(weights):
            # ê°€ì¤‘ ì‹ í˜¸ ìƒì„±
            combined_signals = self._combine_signals(
                regime_data['signals'],
                dict(zip(strategies, weights))
            )

            # ë°±í…ŒìŠ¤íŠ¸ ì„±ê³¼
            performance = self._quick_backtest(combined_signals, regime_data['prices'])

            # ìµœëŒ€í™” â†’ ìµœì†Œí™” ë³€í™˜
            return -performance['sharpe_ratio']

        # ì œì•½: í•© = 1
        constraints = {'type': 'eq', 'fun': lambda w: sum(w) - 1}
        bounds = [(self.min_weight, self.max_weight) for _ in range(n)]

        # ì´ˆê¸°ê°’
        x0 = [1/n] * n

        result = minimize(objective, x0, method='SLSQP',
                         bounds=bounds, constraints=constraints)

        return dict(zip(strategies, result.x))
```

### 3.3 ì§€í‘œ íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”

```python
class IndicatorParameterOptimizer:
    """
    ê¸°ìˆ ì  ì§€í‘œ íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”

    ìµœì í™” ëŒ€ìƒ:
    - RSI ê¸°ê°„, ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ë ˆë²¨
    - MACD íŒŒë¼ë¯¸í„°
    - ì´ë™í‰ê·  ê¸°ê°„
    - ë³¼ë¦°ì €ë°´ë“œ íŒŒë¼ë¯¸í„°
    """

    def optimize_parameters(self,
                           historical_data: pd.DataFrame,
                           indicator: str,
                           param_ranges: dict) -> OptimizedParams:
        """
        ê·¸ë¦¬ë“œ ì„œì¹˜ + ì›Œí¬í¬ì›Œë“œë¡œ íŒŒë¼ë¯¸í„° ìµœì í™”
        """
        best_params = None
        best_score = -np.inf

        # ê·¸ë¦¬ë“œ ìƒì„±
        param_grid = self._create_grid(param_ranges)

        for params in param_grid:
            # ì›Œí¬í¬ì›Œë“œ ê²€ì¦
            scores = self._walk_forward_test(
                historical_data, indicator, params
            )

            avg_score = np.mean(scores)

            if avg_score > best_score:
                best_score = avg_score
                best_params = params

        return OptimizedParams(
            indicator=indicator,
            parameters=best_params,
            score=best_score,
            validation_method='walk_forward'
        )

    def _walk_forward_test(self,
                          data: pd.DataFrame,
                          indicator: str,
                          params: dict,
                          n_splits: int = 5) -> List[float]:
        """
        ì›Œí¬í¬ì›Œë“œ í…ŒìŠ¤íŠ¸

        ê³¼ìµœì í™” ë°©ì§€ë¥¼ ìœ„í•œ ë¡¤ë§ ê²€ì¦
        """
        scores = []
        split_size = len(data) // (n_splits + 1)

        for i in range(n_splits):
            # í•™ìŠµ ê¸°ê°„: ì²˜ìŒ ~ i+1 êµ¬ê°„
            train_end = (i + 1) * split_size
            train_data = data.iloc[:train_end]

            # ê²€ì¦ ê¸°ê°„: i+1 ~ i+2 êµ¬ê°„
            test_start = train_end
            test_end = (i + 2) * split_size
            test_data = data.iloc[test_start:test_end]

            # í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¡œ ì§€í‘œ ê³„ì‚° ë° ì„±ê³¼ ì¸¡ì •
            score = self._evaluate_indicator(
                test_data, indicator, params
            )
            scores.append(score)

        return scores
```

---

## 4. ì‹œì¥ ì ì‘ í•™ìŠµ

### 4.1 ì‹œì¥ ë ˆì§ ê°ì§€ í•™ìŠµ

```python
class RegimeDetectorLearner:
    """
    ì‹œì¥ ë ˆì§ ê°ì§€ ëª¨ë¸ í•™ìŠµ

    ë ˆì§ ë¶„ë¥˜:
    - BULL: ìƒìŠ¹ì¥
    - BEAR: í•˜ë½ì¥
    - RANGE: íš¡ë³´ì¥
    - HIGH_VOL: ê³ ë³€ë™ì„±
    """

    def __init__(self):
        self.features = [
            'market_return_20d',    # 20ì¼ ì‹œì¥ ìˆ˜ìµë¥ 
            'market_volatility',     # ë³€ë™ì„±
            'trend_strength',        # ì¶”ì„¸ ê°•ë„ (ADX)
            'breadth',              # ì‹œì¥ í­ (ìƒìŠ¹ ì¢…ëª© ë¹„ìœ¨)
            'vix_level',            # VIX
            'volume_trend',         # ê±°ë˜ëŸ‰ ì¶”ì„¸
        ]

    def train_regime_classifier(self,
                               market_data: pd.DataFrame,
                               labeled_regimes: pd.Series) -> RegimeClassifier:
        """
        ë ˆì§ ë¶„ë¥˜ê¸° í•™ìŠµ

        ëª¨ë¸: Random Forest (í•´ì„ ê°€ëŠ¥ì„±)
        """
        # í”¼ì²˜ ìƒì„±
        X = self._create_regime_features(market_data)
        y = labeled_regimes

        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False  # ì‹œê³„ì—´ì´ë¯€ë¡œ ì…”í”Œ X
        )

        # ëª¨ë¸ í•™ìŠµ
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=5,  # ê³¼ì í•© ë°©ì§€
            min_samples_leaf=20
        )
        model.fit(X_train, y_train)

        # í‰ê°€
        accuracy = model.score(X_test, y_test)

        # í”¼ì²˜ ì¤‘ìš”ë„
        feature_importance = dict(zip(self.features, model.feature_importances_))

        return RegimeClassifier(
            model=model,
            accuracy=accuracy,
            feature_importance=feature_importance
        )

    def update_regime_labels(self,
                            market_data: pd.DataFrame,
                            actual_performance: pd.DataFrame) -> pd.Series:
        """
        ì‹¤ì œ ì„±ê³¼ ê¸°ë°˜ ë ˆì§ ë ˆì´ë¸” ì—…ë°ì´íŠ¸

        ì„±ê³¼ê°€ ì¢‹ì•˜ë˜ ê¸°ê°„ì˜ ë ˆì§ì„ "ë§ëŠ” ë ˆì§"ìœ¼ë¡œ í•™ìŠµ
        """
        # í˜„ì¬ ë ˆì§ ì˜ˆì¸¡
        predicted_regimes = self.current_model.predict(market_data)

        # ì‹¤ì œ ì„±ê³¼ì™€ ë¹„êµ
        for i, regime in enumerate(predicted_regimes):
            expected_strategy = self.regime_strategies[regime]
            actual_return = actual_performance.iloc[i]

            # ì„±ê³¼ê°€ ê¸°ëŒ€ì— ëª» ë¯¸ì¹˜ë©´ ë ˆì§ ì¬ë¶„ë¥˜ ê³ ë ¤
            if actual_return < expected_strategy['min_return']:
                # ë‹¤ë¥¸ ë ˆì§ì´ì—ˆì„ ê°€ëŠ¥ì„± ì²´í¬
                alternative_regime = self._find_better_regime(
                    market_data.iloc[i], actual_return
                )
                if alternative_regime:
                    predicted_regimes[i] = alternative_regime

        return predicted_regimes
```

### 4.2 ì „ëµ ìë™ ì„ íƒ í•™ìŠµ

```python
class StrategySelector:
    """
    ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ì „ëµ ìë™ ì„ íƒ

    í•™ìŠµ ëª©í‘œ:
    - ì–´ë–¤ ì‹œì¥ì—ì„œ ì–´ë–¤ ì „ëµì´ ìµœì ì¸ì§€ í•™ìŠµ
    - ì „ëµ ì „í™˜ íƒ€ì´ë° í•™ìŠµ
    """

    def __init__(self):
        self.strategies = {
            'momentum': MomentumStrategy(),
            'mean_reversion': MeanReversionStrategy(),
            'trend_following': TrendFollowingStrategy(),
            'defensive': DefensiveStrategy(),
        }

        # ì „ëµë³„ ì í•© ë ˆì§ (ì´ˆê¸°ê°’, í•™ìŠµìœ¼ë¡œ ì—…ë°ì´íŠ¸)
        self.strategy_regime_fit = {
            'momentum': [MarketRegime.BULL],
            'mean_reversion': [MarketRegime.RANGE],
            'trend_following': [MarketRegime.BULL, MarketRegime.BEAR],
            'defensive': [MarketRegime.BEAR, MarketRegime.HIGH_VOL],
        }

    def learn_strategy_selection(self,
                                historical_regimes: List[MarketRegime],
                                strategy_performances: Dict[str, Dict]) -> None:
        """
        ë ˆì§ë³„ ìµœì  ì „ëµ í•™ìŠµ

        ê° ë ˆì§ì—ì„œ ê° ì „ëµì˜ ì„±ê³¼ ë¶„ì„ í›„ ë§¤í•‘ ì—…ë°ì´íŠ¸
        """
        regime_strategy_scores = defaultdict(dict)

        for regime in set(historical_regimes):
            for strategy_name, perf in strategy_performances.items():
                # í•´ë‹¹ ë ˆì§ ê¸°ê°„ì˜ ì „ëµ ì„±ê³¼
                regime_perf = self._filter_performance_by_regime(
                    perf, historical_regimes, regime
                )

                if len(regime_perf) < 10:  # ìµœì†Œ ìƒ˜í”Œ
                    continue

                # ì„±ê³¼ ì ìˆ˜
                score = (
                    regime_perf['sharpe_ratio'] * 0.4 +
                    regime_perf['win_rate'] * 0.3 +
                    (1 - abs(regime_perf['max_drawdown'])) * 0.3
                )

                regime_strategy_scores[regime][strategy_name] = score

        # ë ˆì§ë³„ ìµœì  ì „ëµ ì—…ë°ì´íŠ¸
        for regime, scores in regime_strategy_scores.items():
            if not scores:
                continue

            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            sorted_strategies = sorted(scores.items(), key=lambda x: x[1], reverse=True)

            # ìƒìœ„ 2ê°œ ì „ëµì„ í•´ë‹¹ ë ˆì§ì— ë§¤í•‘
            best_strategies = [s[0] for s in sorted_strategies[:2]]

            # ë§¤í•‘ ì—…ë°ì´íŠ¸
            self.strategy_regime_fit = self._update_regime_mapping(
                self.strategy_regime_fit, regime, best_strategies
            )

    def select_strategy(self, current_regime: MarketRegime) -> str:
        """
        í˜„ì¬ ë ˆì§ì— ìµœì ì¸ ì „ëµ ì„ íƒ
        """
        # í•´ë‹¹ ë ˆì§ì— ì í•©í•œ ì „ëµë“¤
        suitable_strategies = []
        for strategy, regimes in self.strategy_regime_fit.items():
            if current_regime in regimes:
                suitable_strategies.append(strategy)

        if not suitable_strategies:
            return 'defensive'  # ê¸°ë³¸ê°’

        # ìµœê·¼ ì„±ê³¼ê°€ ê°€ì¥ ì¢‹ì€ ì „ëµ ì„ íƒ
        best_strategy = max(
            suitable_strategies,
            key=lambda s: self._get_recent_performance(s)
        )

        return best_strategy
```

---

## 5. í•™ìŠµ ì£¼ê¸° ë° ìë™í™”

### 5.1 í•™ìŠµ ìŠ¤ì¼€ì¤„

```python
class LearningScheduler:
    """
    í•™ìŠµ ì‘ì—… ìŠ¤ì¼€ì¤„ ê´€ë¦¬

    ìŠ¤ì¼€ì¤„:
    - ì‹¤ì‹œê°„: ê±°ë˜ ë¡œê·¸ ìˆ˜ì§‘
    - ì¼ê°„: ë‹¹ì¼ ì„±ê³¼ ë¶„ì„
    - ì£¼ê°„: ê°€ì¤‘ì¹˜ ì¡°ì •, íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
    - ì›”ê°„: ëª¨ë¸ ì¬í•™ìŠµ, ì „ëµ ì¬í‰ê°€
    - ë¶„ê¸°: ì „ì²´ ì‹œìŠ¤í…œ ë¦¬ë·°
    """

    def __init__(self):
        self.schedule = {
            'realtime': [
                {'task': 'log_trade', 'trigger': 'on_trade'},
                {'task': 'update_metrics', 'trigger': 'on_price_update'},
            ],
            'daily': [
                {'task': 'daily_performance_analysis', 'time': '16:00'},
                {'task': 'failure_analysis', 'time': '16:30'},
                {'task': 'update_regime', 'time': '17:00'},
            ],
            'weekly': [
                {'task': 'adjust_ensemble_weights', 'day': 'friday', 'time': '17:00'},
                {'task': 'optimize_indicator_params', 'day': 'saturday', 'time': '10:00'},
                {'task': 'sector_rotation_update', 'day': 'sunday', 'time': '18:00'},
            ],
            'monthly': [
                {'task': 'retrain_lstm', 'day': 1, 'time': '06:00'},
                {'task': 'strategy_performance_review', 'day': 1, 'time': '10:00'},
                {'task': 'regime_classifier_update', 'day': 15, 'time': '06:00'},
            ],
            'quarterly': [
                {'task': 'full_system_review', 'month': [1, 4, 7, 10], 'day': 1},
                {'task': 'backtest_all_strategies', 'month': [1, 4, 7, 10], 'day': 2},
            ]
        }

    def run_scheduled_tasks(self):
        """
        ìŠ¤ì¼€ì¤„ëœ í•™ìŠµ ì‘ì—… ì‹¤í–‰
        """
        current_time = datetime.now()

        # ì¼ê°„ ì‘ì—…
        for task in self.schedule['daily']:
            if self._should_run(task, current_time):
                self._execute_task(task['task'])

        # ì£¼ê°„ ì‘ì—…
        for task in self.schedule['weekly']:
            if self._should_run_weekly(task, current_time):
                self._execute_task(task['task'])

        # ì›”ê°„ ì‘ì—…
        for task in self.schedule['monthly']:
            if self._should_run_monthly(task, current_time):
                self._execute_task(task['task'])
```

### 5.2 í•™ìŠµ ê²°ê³¼ ì¶”ì 

```python
class LearningTracker:
    """
    í•™ìŠµ ê²°ê³¼ ì¶”ì  ë° ë²„ì „ ê´€ë¦¬

    ì¶”ì  í•­ëª©:
    - ëª¨ë¸ ë²„ì „
    - íŒŒë¼ë¯¸í„° ë³€ê²½ ì´ë ¥
    - ì„±ê³¼ ë³€í™” ì¶”ì´
    """

    def __init__(self):
        self.db = LearningDatabase()

    def log_learning_result(self, result: LearningResult) -> None:
        """
        í•™ìŠµ ê²°ê³¼ ê¸°ë¡
        """
        record = LearningRecord(
            timestamp=datetime.now(),
            learning_type=result.type,  # 'model_retrain', 'weight_adjust', etc.

            # ë³€ê²½ ì „/í›„ ìƒíƒœ
            before_state=result.before_state,
            after_state=result.after_state,

            # ì„±ê³¼ ì§€í‘œ
            before_performance=result.before_performance,
            after_performance=result.after_performance,
            improvement=result.improvement,

            # ë©”íƒ€ë°ì´í„°
            training_samples=result.training_samples,
            validation_score=result.validation_score,
            notes=result.notes
        )

        self.db.save(record)

    def get_learning_history(self,
                            learning_type: str = None,
                            days: int = 30) -> List[LearningRecord]:
        """
        í•™ìŠµ ì´ë ¥ ì¡°íšŒ
        """
        return self.db.query(
            learning_type=learning_type,
            since=datetime.now() - timedelta(days=days)
        )

    def analyze_learning_effectiveness(self) -> LearningEffectivenessReport:
        """
        í•™ìŠµ íš¨ê³¼ ë¶„ì„

        ì§ˆë¬¸:
        - í•™ìŠµ í›„ ì„±ê³¼ê°€ ê°œì„ ë˜ì—ˆëŠ”ê°€?
        - ì–´ë–¤ ìœ í˜•ì˜ í•™ìŠµì´ ê°€ì¥ íš¨ê³¼ì ì¸ê°€?
        - í•™ìŠµ ì£¼ê¸°ëŠ” ì ì ˆí•œê°€?
        """
        history = self.get_learning_history(days=90)

        return LearningEffectivenessReport(
            # í•™ìŠµ ìœ í˜•ë³„ íš¨ê³¼
            effectiveness_by_type={
                learning_type: self._analyze_type_effectiveness(
                    [r for r in history if r.learning_type == learning_type]
                )
                for learning_type in set(r.learning_type for r in history)
            },

            # ì „ì²´ ê°œì„ ìœ¨
            overall_improvement=self._calculate_overall_improvement(history),

            # ê¶Œì¥ ì‚¬í•­
            recommendations=self._generate_recommendations(history)
        )
```

---

## 6. í•™ìŠµ ì•ˆì „ì¥ì¹˜

### 6.1 ê³¼ì í•© ë°©ì§€

```python
class OverfitPrevention:
    """
    í•™ìŠµ ê³¼ì í•© ë°©ì§€ ì¥ì¹˜

    ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜:
    1. ì›Œí¬í¬ì›Œë“œ ê²€ì¦ í•„ìˆ˜
    2. ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ìš”êµ¬
    3. ì„±ê³¼ ë³€í™” ì œí•œ
    4. A/B í…ŒìŠ¤íŠ¸
    """

    def validate_learning_result(self, result: LearningResult) -> ValidationResult:
        """
        í•™ìŠµ ê²°ê³¼ ê²€ì¦
        """
        checks = []

        # 1. ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        if result.training_samples < 100:
            checks.append(ValidationCheck(
                passed=False,
                check='min_samples',
                message=f"ìƒ˜í”Œ ë¶€ì¡±: {result.training_samples} < 100"
            ))
        else:
            checks.append(ValidationCheck(passed=True, check='min_samples'))

        # 2. ê³¼ì í•© ì§•í›„ (í•™ìŠµ/ê²€ì¦ ì„±ê³¼ ì°¨ì´)
        train_val_gap = result.training_score - result.validation_score
        if train_val_gap > 0.1:  # 10% ì´ìƒ ì°¨ì´
            checks.append(ValidationCheck(
                passed=False,
                check='overfit_gap',
                message=f"ê³¼ì í•© ì˜ì‹¬: í•™ìŠµ/ê²€ì¦ ì°¨ì´ {train_val_gap:.1%}"
            ))
        else:
            checks.append(ValidationCheck(passed=True, check='overfit_gap'))

        # 3. ê¸‰ê²©í•œ ë³€í™”
        if abs(result.improvement) > 0.3:  # 30% ì´ìƒ ë³€í™”
            checks.append(ValidationCheck(
                passed=False,
                check='sudden_change',
                message=f"ê¸‰ê²©í•œ ë³€í™”: {result.improvement:.1%}"
            ))
        else:
            checks.append(ValidationCheck(passed=True, check='sudden_change'))

        # 4. Out-of-sample ì„±ê³¼
        if result.out_of_sample_score < result.validation_score * 0.8:
            checks.append(ValidationCheck(
                passed=False,
                check='oos_performance',
                message="Out-of-sample ì„±ê³¼ ì €ì¡°"
            ))
        else:
            checks.append(ValidationCheck(passed=True, check='oos_performance'))

        all_passed = all(c.passed for c in checks)

        return ValidationResult(
            passed=all_passed,
            checks=checks,
            recommendation='apply' if all_passed else 'reject'
        )
```

### 6.2 ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜

```python
class ModelRollback:
    """
    í•™ìŠµ ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜

    ë¡¤ë°± ì¡°ê±´:
    - ìƒˆ ëª¨ë¸ ì ìš© í›„ ì„±ê³¼ ê¸‰ê²©íˆ ì•…í™”
    - ì—°ì† 3íšŒ ì´ìƒ ì†ì‹¤
    - ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ
    """

    def __init__(self):
        self.model_history = []  # ëª¨ë¸ ë²„ì „ íˆìŠ¤í† ë¦¬
        self.performance_window = 10  # ì„±ê³¼ í‰ê°€ ê¸°ê°„

    def should_rollback(self,
                       current_model_performance: float,
                       previous_model_performance: float) -> RollbackDecision:
        """
        ë¡¤ë°± í•„ìš” ì—¬ë¶€ íŒë‹¨
        """
        performance_drop = previous_model_performance - current_model_performance

        # ì¡°ê±´ 1: ì„±ê³¼ ê¸‰ë½ (20% ì´ìƒ)
        if performance_drop > 0.2:
            return RollbackDecision(
                should_rollback=True,
                reason=f"ì„±ê³¼ ê¸‰ë½: {performance_drop:.1%}",
                target_version=self._get_previous_version()
            )

        # ì¡°ê±´ 2: ì—°ì† ì†ì‹¤
        recent_trades = self._get_recent_trades(self.performance_window)
        consecutive_losses = self._count_consecutive_losses(recent_trades)

        if consecutive_losses >= 5:
            return RollbackDecision(
                should_rollback=True,
                reason=f"ì—°ì† {consecutive_losses}íšŒ ì†ì‹¤",
                target_version=self._get_previous_version()
            )

        return RollbackDecision(should_rollback=False)

    def rollback(self, target_version: str) -> bool:
        """
        ì´ì „ ëª¨ë¸ë¡œ ë¡¤ë°±
        """
        previous_model = self._load_model_version(target_version)

        if previous_model is None:
            return False

        # í˜„ì¬ ëª¨ë¸ ë°±ì—…
        self._backup_current_model()

        # ë¡¤ë°± ì‹¤í–‰
        self.current_model = previous_model

        # ì•Œë¦¼
        self._send_rollback_notification(target_version)

        return True
```

---

## 7. êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: ê±°ë˜ í•™ìŠµ (Week 1)
```
1. core/learning/trade_logger.py        - ê±°ë˜ ë¡œê·¸ ìˆ˜ì§‘
2. core/learning/performance_analyzer.py - ì„±ê³¼ íŒ¨í„´ ë¶„ì„
3. core/learning/failure_analyzer.py    - ì‹¤íŒ¨ ë¶„ì„
```

### Phase 2: ëª¨ë¸ í•™ìŠµ (Week 2)
```
1. core/learning/lstm_learner.py        - LSTM ì¬í•™ìŠµ
2. core/learning/weight_learner.py      - ì•™ìƒë¸” ê°€ì¤‘ì¹˜ í•™ìŠµ
3. core/learning/param_optimizer.py     - íŒŒë¼ë¯¸í„° ìµœì í™”
```

### Phase 3: ì‹œì¥ ì ì‘ (Week 3)
```
1. core/learning/regime_learner.py      - ë ˆì§ ê°ì§€ í•™ìŠµ
2. core/learning/strategy_selector.py   - ì „ëµ ì„ íƒ í•™ìŠµ
```

### Phase 4: ìë™í™” (Week 4)
```
1. core/learning/scheduler.py           - í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬
2. core/learning/tracker.py             - í•™ìŠµ ì¶”ì 
3. core/learning/safety.py              - ì•ˆì „ì¥ì¹˜
```

---

*ë‹¤ìŒ ë¬¸ì„œ: `04_IMPLEMENTATION_CHECKLIST.md` - êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸*
